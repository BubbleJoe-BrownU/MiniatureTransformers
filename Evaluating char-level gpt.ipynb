{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d2c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705f3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "sys.path.insert(0, 'models')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd25f0",
   "metadata": {},
   "source": [
    "# Load model args and state dict from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e591d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model was trained on a GeForce 3060 Ti GPU for 5000 steps with a batch size of 16\n",
    "checkpoint_pt = 'out-shakespeare-char/ckpt.pt'\n",
    "checkpoint = torch.load(checkpoint_pt)\n",
    "model_state_dict = checkpoint['model']\n",
    "model_args = checkpoint['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c34679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to modify the state_dict a bit if they were saved from a compiled model\n",
    "# pytorch automatically adds a prefix of _orig_mod. to keys in state dict if the model has been compiled\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(model_state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        model_state_dict[k[len(unwanted_prefix):]] = model_state_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7537e979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniGPT(\n",
       "  (input_embedding): Embedding(65, 768)\n",
       "  (position_embedding): LearnablePositionEmbedding(\n",
       "    (PE): Embedding(128, 768)\n",
       "  )\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (decoder): ModuleList(\n",
       "    (0-11): 12 x DecoderLayer(\n",
       "      (self_attn): MultiHeadSelfAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-11): 12 x SelfAttentionHead(\n",
       "            (M_key): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (M_query): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (M_value): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DecoderModels import MiniGPT\n",
    "\n",
    "model = MiniGPT(**model_args)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f042ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 12,\n",
       " 'num_heads': 12,\n",
       " 'embed_size': 768,\n",
       " 'ctx_length': 128,\n",
       " 'bias': False,\n",
       " 'vocab_size': 65,\n",
       " 'dropout': 0.2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71fd49",
   "metadata": {},
   "source": [
    "## Load dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1ed203",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.memmap('data/shakespeare_char/val.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "with open('data/shakespeare_char/meta.pkl', 'rb') as m:\n",
    "    meta = pickle.load(m)\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([meta['itos'][i] for i in l])\n",
    "    \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78433a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_length = model_args['ctx_length']\n",
    "batch_size = 16\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    data_len = (len(data) - 1) // ctx_length * ctx_length + 1\n",
    "    data = data[:data_len]\n",
    "    chunk_size = batch_size * ctx_length\n",
    "    for i in range(0, data_len, chunk_size):\n",
    "        end = min(data_len-1, i+chunk_size)\n",
    "        x = torch.from_numpy(data[i:end].astype(np.int64)).reshape(-1, ctx_length)\n",
    "        y = torch.from_numpy(data[i+1:end+1].astype(np.int64)).reshape(-1, ctx_length)\n",
    "#         x = torch.stack([torch.from_numpy((data[i:i+ctx_length]).astype(np.int64)) for i in ix])\n",
    "#         y = torch.stack([torch.from_numpy((data[i+1:i+1+ctx_length]).astype(np.int64)) for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aa373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc12ee87",
   "metadata": {},
   "source": [
    "## generate some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7003a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That shall she shall dine thee from the world.\n",
      "\n",
      "Second Servingman:\n",
      "The heavens to see the constate o\n",
      "**********\n",
      "\n",
      "Thou and transmen on and this sill-day of mark,\n",
      "That his stones to the world.\n",
      "\n",
      "Servant:\n",
      "Well, what w\n",
      "**********\n",
      "\n",
      "And marry a murdder these forghasts, and tender former lights.\n",
      "\n",
      "GLOUCESTER:\n",
      "How far, teach the truth\n",
      "**********\n",
      "\n",
      "I'll sue task of a fight tidow trimpets the feast\n",
      "Whatsoe's bust aganst to burnet, an order, stear o\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "text = '\\n'\n",
    "seq = torch.Tensor([encode(text)]).to(torch.long).to('cuda')\n",
    "for t in [0.5, 1.0, 1.5, 2.0]:\n",
    "    result = model.generate(seq, 100, temperature=t, top_k=5)\n",
    "    print(decode(result[0].tolist()))\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348832fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These violent delights, which they have made\n",
      "Their fellows from their faces of their friends,\n",
      "And their father was so far \n",
      "**********\n",
      "These violent delights of the priest.\n",
      "\n",
      "Lord Mayor:\n",
      "And where is he then so free thine eyes,\n",
      "The date of hope of traitors, \n",
      "**********\n",
      "These violent delights, and wert the foest of speak triumph\n",
      "That wash him they hear; what worst their flant,\n",
      "Both, at her \n",
      "**********\n",
      "These violent delights arit,\n",
      "Bud thee, and thy,' taken, to be so,--all trees!--\n",
      "They was declined without, at\n",
      "Time shoroug\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "text = 'These violent delights'\n",
    "seq = torch.Tensor([encode(text)]).to(torch.long).to('cuda')\n",
    "for t in [0.5, 1.0, 1.5, 2.0]:\n",
    "    result = model.generate(seq, 100, temperature=t, top_k=5)\n",
    "    print(decode(result[0].tolist()))\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f576111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be so.\n",
      "\n",
      "KING RICHARD II:\n",
      "So did I see the king of his honours\n",
      "That hath beheld the heart of his highne\n",
      "**********\n",
      "To be or not to be a good of soul,\n",
      "To seek a fault, as it is that are alms,\n",
      "That thou shalt have something the happy d\n",
      "**********\n",
      "To be or not to beet holy trust again\n",
      "The strange of the solemness of all\n",
      "Of treacons them, all their side arm,\n",
      "Turn'd\n",
      "**********\n",
      "To be or not to be set in somewharges. Then\n",
      "still weeps your friendlot-peaction flocks me at tears\n",
      "attends:--\n",
      "\n",
      "POLIMER\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "text = 'To be or not to be'\n",
    "seq = torch.Tensor([encode(text)]).to(torch.long).to('cuda')\n",
    "for t in [0.5, 1.0, 1.5, 2.0]:\n",
    "    result = model.generate(seq, 100, temperature=t, top_k=5)\n",
    "    print(decode(result[0].tolist()))\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcbfd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like the movie Barbies and means,\n",
      "To see the firest speech of the world:\n",
      "We are not the crown for the fiery son,\n",
      "And the \n",
      "**********\n",
      "I like the movie Barbies and fearful that trence\n",
      "In this forty brats any out.\n",
      "\n",
      "POLIXENES:\n",
      "O, with some foemile, will not st\n",
      "**********\n",
      "I like the movie Barbies\n",
      "To leave it the wenches, and stale to tasquisities,--\n",
      "\n",
      "CATESBY:\n",
      "And, stay, mine own lords thapes t\n",
      "**********\n",
      "I like the movie Barbie thine;\n",
      "Alack, a voint, be groat and loog,\n",
      "Will so fail, as I am, is at,--\n",
      "Than alrah o'er y'er hast\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "text = 'I like the movie Barbie'\n",
    "seq = torch.Tensor([encode(text)]).to(torch.long).to('cuda')\n",
    "for t in [0.5, 1.0, 1.5, 2.0]:\n",
    "    result = model.generate(seq, 100, temperature=t, top_k=5)\n",
    "    print(decode(result[0].tolist()))\n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72521d2",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71f987c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average model loss on the validation split is: 1.484342564236034\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_batch('val')\n",
    "losses = []\n",
    "perplexity = 0\n",
    "model.eval()\n",
    "for inputs, labels in dataloader:\n",
    "    _, loss = model(inputs, labels)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "losses = np.array(losses)\n",
    "print(f\"Average model loss on the validation split is: {losses.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3d8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
